---
title: "Configure LiteLLM Provider (Admin)"
sidebarTitle: "Configure LiteLLM (Admin)"
description: "This guide explains how administrators configure LiteLLM as the organization-wide LLM provider for Cline."
---

As an administrator, you can add LiteLLM as the organization-wide LLM provider for all Cline users. This centralized approach provides unified access to 100+ AI models through a single proxy interface while maintaining your organization's cost controls, model routing policies, and usage analytics.

## Before You Begin

To get started with setting up LiteLLM as your organization's LLM provider, you'll need a few items in place.

**Administrator access to the Cline Admin console**  
You need admin privileges to enforce provider settings across your organization. If you can navigate to **Settings → Cline Settings** in the admin console at [app.cline.bot](https://app.cline.bot), you have the right access level.

<Info>
**Quick Check**: Try accessing the settings page now. If you can see the provider configuration options, you're good to go.
</Info>

**LiteLLM proxy instance running**  
You need a deployed LiteLLM proxy that your team can access. This can be self-hosted or managed through a cloud provider.

<Note>
If you haven't deployed LiteLLM yet, check the [Enterprise Infrastructure Setup](/enterprise-solutions/enterprise-configurations/providers/litellm/overview) for comprehensive deployment guidance.
</Note>

**LiteLLM master key**  
You'll need the master key for your LiteLLM deployment to configure authentication and model access.

<Tip>
Ensure your LiteLLM proxy is accessible from your team's development environments and has the models you want to make available configured.
</Tip>

<Frame>
	<img
		src="https://storage.googleapis.com/cline-static-assets-prod/assets/LiteLLM%20Remote%20Config.gif"
	/>
</Frame>

## Configuration Steps

<Steps>
<Step title="Access Cline Settings">
Navigate to [app.cline.bot](https://app.cline.bot) and sign in with your administrator account. Go to **Settings → Cline Settings**.

<Info>
You should see the provider configuration options if you have the correct admin access level.
</Info>
</Step>

<Step title="Enable Remote Provider Configuration">
Toggle on **Enable settings** to reveal the remote provider configuration options. This allows you to enforce provider settings across your organization.
</Step>

<Step title="Select LiteLLM as the API Provider">
Open the **API Provider** dropdown menu and select **LiteLLM**. This will open the LiteLLM configuration panel where you'll configure all your organization-wide settings.
</Step>

<Step title="Configure LiteLLM Settings">
The configuration panel includes several settings that control how LiteLLM works for your organization:

<AccordionGroup>
<Accordion title="Base URL (required)">
Enter your LiteLLM proxy endpoint URL. This should be the full URL where your LiteLLM proxy is accessible, such as `https://litellm.yourcompany.com` or `http://your-proxy:4000`.

<Tip>
Use HTTPS endpoints in production for security. Make sure the URL is accessible from your team's development environments.
</Tip>
</Accordion>

<Accordion title="Master Key (optional)">
If your LiteLLM proxy requires authentication, enter the master key here. This will be used to authenticate requests from all organization members.

<Warning>
The master key provides full access to your LiteLLM proxy. Only enter this if your proxy requires authentication and you want centralized key management.
</Warning>
</Accordion>

<Accordion title="Model Selection (optional)">
Configure which models are available to your organization. You can either allow all models configured in your LiteLLM proxy, or restrict to specific models.

[View LiteLLM Model List](https://litellm.vercel.app/docs/providers)
</Accordion>

<Accordion title="Request Routing (optional)">
Configure routing preferences for model requests, including fallback models and load balancing strategies if supported by your LiteLLM deployment.
</Accordion>

<Accordion title="Cost Controls (optional)">
Set up budget limits and usage monitoring if your LiteLLM deployment supports cost tracking features.
</Accordion>
</AccordionGroup>
</Step>

<Step title="Save Configuration">
After configuring your settings, close the provider configuration panel and click **Save** on the settings page to persist your changes.

Once saved, all organization members signed into the Cline extension will automatically use LiteLLM with your configured settings. They won't be able to select other providers or switch to their personal Cline accounts.

<Warning>
Members can't switch to personal Cline accounts or join other organizations once remote configuration is enabled. This ensures consistent provider usage across your team.
</Warning>
</Step>
</Steps>

## Verification

To verify the configuration:

1. Check that the provider shows as "LiteLLM" in the Enabled provider field
2. Confirm the settings persist after refreshing the page
3. Test with a member account to ensure they see only LiteLLM as a provider
4. Verify that the configured models are available in the model dropdown

## Model Management

### Available Models
The models available through your remote LiteLLM configuration depend on your proxy setup. Common model categories include:

- **OpenAI Models**: GPT-4, GPT-3.5-turbo variants
- **Anthropic Models**: Claude 3 Sonnet, Claude 3 Haiku, Claude 3 Opus  
- **Open Source Models**: Llama 2, Mistral, CodeLlama
- **Specialized Models**: Code generation, embedding, image analysis models

### Model Configuration
Configure available models in your organization through:

1. **LiteLLM Proxy Configuration**: Add/remove models in your proxy's model list
2. **Remote Configuration**: Restrict which models are visible to your team
3. **Real-time Updates**: Changes to model availability can be updated without restarting

## Cost Management

### Usage Tracking
If your LiteLLM deployment includes usage tracking:

- **Per-User Monitoring**: Track usage by individual team members
- **Model-Level Analytics**: See which models are most popular
- **Cost Attribution**: Allocate costs across teams or projects
- **Budget Alerts**: Set up notifications for usage thresholds

### Budget Controls  
Configure spending limits through the admin console:

- **Monthly Budgets**: Set organization-wide spending limits
- **Per-Model Limits**: Control access to expensive models
- **User Quotas**: Limit individual team member usage
- **Automatic Cutoffs**: Stop requests when budgets are exceeded

## Troubleshooting

**Members don't see the configured provider**  
Ensure you clicked Save after closing the configuration panel. Verify the member account belongs to the correct organization and that your LiteLLM proxy is accessible from their network.

**Connection errors to LiteLLM proxy**  
Verify the Base URL is correct and accessible. Check that any firewalls or security groups allow access from your team's IP addresses.

**Authentication failures**  
If using a master key, verify it's correctly entered and has proper permissions in your LiteLLM deployment. Check the LiteLLM proxy logs for authentication errors.

**Models not available**  
Confirm the models are properly configured in your LiteLLM proxy. Check that model-specific API keys are valid and that the models are not rate-limited.

**Configuration changes don't persist**  
Make sure to click the Save button on the main settings page, not just close the configuration panel.

