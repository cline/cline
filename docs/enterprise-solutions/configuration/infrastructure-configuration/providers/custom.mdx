---
title: "Custom Provider Configuration"
sidebarTitle: "Custom Providers"
description: "Configure custom OpenAI-compatible providers for your Cline deployment"
---

<Info>
**Configuration Path: Self-Hosted**

This guide covers custom provider configuration for self-hosted deployments.
</Info>

Configure Cline to use custom OpenAI-compatible API providers, including Azure OpenAI, self-hosted models, and other third-party services.

## What are Custom Providers?

Custom providers include any API that implements the OpenAI API format:

- **Azure OpenAI Service**: Microsoft's managed OpenAI models
- **vLLM**: Self-hosted inference server
- **Ollama**: Local model runner
- **Text Generation Inference**: Hugging Face's inference server
- **FastChat**: Open-source chatbot platform
- **LocalAI**: Local OpenAI API replacement
- **Custom APIs**: Your own OpenAI-compatible implementations

## Configuration

Configure custom providers through your remote configuration JSON:

```json
{
  "providerSettings": {
    "provider": "openai",
    "openaiBaseUrl": "https://your-api.company.com/v1",
    "openaiApiKey": "your-api-key"
  }
}
```

### Configuration Options

| Field | Description | Required |
|-------|-------------|----------|
| `provider` | Use `"openai"` for OpenAI-compatible APIs | Yes |
| `openaiBaseUrl` | Base URL of your API endpoint | Yes |
| `openaiApiKey` | API key for authentication | Yes |
| `openaiModelId` | Model identifier (if different from default) | No |

## Common Custom Providers

### Azure OpenAI

Configure Cline to use Azure OpenAI Service:

```json
{
  "providerSettings": {
    "provider": "azure",
    "azureApiKey": "your-azure-key",
    "azureBaseUrl": "https://your-resource.openai.azure.com",
    "azureDeployment": "gpt-4-deployment"
  }
}
```

### Self-Hosted vLLM

Point to a self-hosted vLLM server:

```json
{
  "providerSettings": {
    "provider": "openai",
    "openaiBaseUrl": "http://vllm.internal.company.com:8000/v1",
    "openaiModelId": "meta-llama/Llama-2-70b-chat"
  }
}
```

### Ollama

Use locally deployed Ollama models:

```json
{
  "providerSettings": {
    "provider": "openai",
    "openaiBaseUrl": "http://localhost:11434/v1",
    "openaiModelId": "codellama"
  }
}
```

### Text Generation Inference (TGI)

Configure for Hugging Face TGI server:

```json
{
  "providerSettings": {
    "provider": "openai",
    "openaiBaseUrl": "http://tgi.company.com:8080/v1",
    "openaiApiKey": "your-tgi-api-key"
  }
}
```

## Authentication

Custom providers support various authentication methods:

### API Key (Standard)

Most providers use API key authentication:
```json
{
  "openaiApiKey": "sk-your-api-key-here"
}
```

### No Authentication

For internal networks without authentication:
```json
{
  "provider": "openai",
  "openaiBaseUrl": "http://internal.api:8000/v1"
}
```

<Note>
Omit the `openaiApiKey` field for providers that don't require authentication.
</Note>

## Model Selection

Specify which model to use:

```json
{
  "providerSettings": {
    "provider": "openai",
    "openaiBaseUrl": "https://api.company.com/v1",
    "openaiModelId": "gpt-4-custom"
  },
  "modelSettings": {
    "defaultModel": "gpt-4-custom"
  }
}
```

## Best Practices

### Security
- **Use HTTPS**: Always use encrypted connections in production
- **Secure API Keys**: Store keys securely, never commit to version control
- **Network Isolation**: Deploy internal APIs on private networks
- **Authentication**: Enable authentication even for internal services

### Performance
- **Proximity**: Deploy services close to your users
- **Load Balancing**: Use load balancers for high availability
- **Timeout Configuration**: Set appropriate timeout values
- **Connection Pooling**: Reuse connections when possible

### Reliability
- **Health Checks**: Monitor endpoint availability
- **Error Handling**: Implement retry logic for transient failures
- **Fallback Providers**: Consider backup providers for critical systems
- **Rate Limiting**: Respect provider rate limits

## Deploying Self-Hosted Models

If you're deploying your own inference servers:

### vLLM (Recommended)

High-performance inference engine:
```bash
docker run --gpus all -p 8000:8000 \
  vllm/vllm-openai:latest \
  --model meta-llama/Llama-2-70b-chat-hf \
  --tensor-parallel-size 4
```

### Ollama

Easy local deployment:
```bash
# Install Ollama
curl https://ollama.ai/install.sh | sh

# Pull and run a model
ollama pull codellama
ollama serve
```

### Text Generation Inference

Hugging Face's inference server:
```bash
docker run --gpus all -p 8080:80 \
  ghcr.io/huggingface/text-generation-inference:latest \
  --model-id meta-llama/Llama-2-70b-chat-hf
```

<Warning>
Deploying and managing inference servers is outside the scope of Cline documentation. Consult the respective project documentation for setup guides.
</Warning>

## Troubleshooting

### Connection Errors

Verify the endpoint is accessible:
```bash
curl https://your-api.company.com/v1/models
```

### Authentication Errors

Test authentication:
```bash
curl -H "Authorization: Bearer your-api-key" \
  https://your-api.company.com/v1/models
```

### Model Not Found

Verify the model ID is correct:
```bash
curl -H "Authorization: Bearer your-api-key" \
  https://your-api.company.com/v1/models
```

### Timeout Issues

Adjust timeout settings if responses are slow:
- Check network latency
- Verify server has adequate resources
- Consider using faster models for time-sensitive operations

## Example Configurations

### Azure OpenAI Production

```json
{
  "providerSettings": {
    "provider": "azure",
    "azureApiKey": "${AZURE_OPENAI_API_KEY}",
    "azureBaseUrl": "https://company-prod.openai.azure.com",
    "azureDeployment": "gpt-4-turbo"
  }
}
```

### Internal vLLM Cluster

```json
{
  "providerSettings": {
    "provider": "openai",
    "openaiBaseUrl": "http://vllm-prod.internal:8000/v1",
    "openaiModelId": "meta-llama/Llama-2-70b-chat-hf"
  }
}
```

### Local Development with Ollama

```json
{
  "providerSettings": {
    "provider": "openai",
    "openaiBaseUrl": "http://localhost:11434/v1",
    "openaiModelId": "codellama"
  }
}
```

## Related Resources

<CardGroup cols={2}>
  <Card title="Azure OpenAI" icon="microsoft" href="https://azure.microsoft.com/en-us/products/ai-services/openai-service">
    Microsoft's managed OpenAI service
  </Card>
  
  <Card title="vLLM" icon="server" href="https://docs.vllm.ai/">
    High-performance inference engine
  </Card>
  
  <Card title="Ollama" icon="download" href="https://ollama.ai/">
    Run models locally
  </Card>
  
  <Card title="Text Generation Inference" icon="code" href="https://huggingface.co/docs/text-generation-inference/">
    Hugging Face inference server
  </Card>
</CardGroup>
