---
title: "Ollama"
description: "A quick guide to setting up Ollama for local AI model execution with Cline."
---

### üìã Prerequisites

-   Windows, macOS, or Linux computer
-   Cline installed in VS Code

### üöÄ Setup Steps

#### 1. Install Ollama

-   Visit [ollama.com](https://ollama.com)
-   Download and install for your operating system

<Frame>
	<img src="/assets/robot_panel_dark.png" alt="Ollama download page" />
</Frame>

#### 2. Choose and Download a Model

-   Browse models at [ollama.com/search](https://ollama.com/search)
-   Select model and copy command:

    ```bash
    ollama run [model-name]
    ```

<Frame>
	<img src="/assets/robot_panel_dark.png" alt="Selecting a model in Ollama" />
</Frame>

-   Open your Terminal and run the command:

    -   Example:

        ```bash
        ollama run llama2
        ```

<Frame>
	<img src="/assets/robot_panel_dark.png" alt="Running Ollama in terminal" />
</Frame>

**‚ú® Your model is now ready to use within Cline!**

#### 3. Configure Cline

1. Open VS Code
2. Click Cline settings icon
3. Select "Ollama" as API provider
4. Enter configuration:
    - Base URL: `http://localhost:11434/` (default value, can be left as is)
    - Select the model from your available options

<Frame>
	<img src="/assets/robot_panel_dark.png" alt="Configuring Cline with Ollama" />
</Frame>

### ‚ö†Ô∏è Important Notes

-   Start Ollama before using with Cline
-   Keep Ollama running in background
-   First model download may take several minutes

### üîß Troubleshooting

If Cline can't connect to Ollama:

1. Verify Ollama is running
2. Check base URL is correct
3. Ensure model is downloaded

Need more info? Read the [Ollama Docs](https://github.com/ollama/ollama/blob/main/docs/api.md).
