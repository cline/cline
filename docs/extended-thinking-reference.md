# Extended Thinking Documentation Reference

## How Extended Thinking Works

When extended thinking is turned on, Claude creates `thinking` content blocks where it outputs its internal reasoning. Claude incorporates insights from this reasoning before crafting a final response.

The API response includes both `thinking` and `text` content blocks.

In multi-turn conversations:
- Only thinking blocks associated with a tool use session or `assistant` turn in the last message position are visible to Claude and billed as input tokens
- Thinking blocks from earlier `assistant` messages are not visible to Claude during sampling and do not get billed as input tokens

## Implementing Extended Thinking

To implement extended thinking:

1. Add the `thinking` parameter and a specified token budget to your API request
2. The `budget_tokens` parameter determines the maximum number of tokens Claude can use for internal reasoning
3. Larger budgets can improve response quality for complex problems
4. `budget_tokens` must always be less than `max_tokens`

Example API request:
```json
{
    "model": "claude-3-7-sonnet-20250219",
    "max_tokens": 20000,
    "thinking": {
        "type": "enabled",
        "budget_tokens": 16000
    }
}
```

## Understanding Thinking Blocks

Thinking blocks represent Claude's internal thought process and have these key characteristics:

1. **Signature Field**
   - Contains a cryptographic token verifying the block was generated by Claude
   - Verified when blocks are passed back to API
   - Added via `signature_delta` in streaming responses

2. **Redacted Thinking**
   - Internal reasoning may be flagged by safety systems
   - Encrypted content returned as `redacted_thinking` block
   - Decrypted when passed back to API
   - Allows response continuation without losing context

3. **Block Order**
   - `thinking` and `redacted_thinking` blocks come before `text` blocks
   - Must maintain exact sequence when passing blocks back to API

## Token Usage and Context Window

### Context Window Calculation
```
context window =
  (current input tokens - previous thinking tokens) +
  (thinking tokens + redacted thinking tokens + text output tokens)
```

Key points:
- Thinking blocks from previous turns are stripped and not counted towards context window
- Current turn thinking counts towards `max_tokens` limit
- Previous thinking tokens don't contribute to input token count for subsequent turns

### Token Management
- Extended thinking tokens count as output tokens
- All thinking tokens (including redacted) count toward rate limits
- Previous turn thinking blocks don't get billed as input tokens
- 28-29 token system prompt included when extended thinking is enabled

### Pricing (Claude 3.7 Sonnet)
| Token use | Cost |
|-----------|------|
| Input tokens | $3 / MTok |
| Output tokens (including thinking) | $15 / MTok |
| Prompt caching write | $3.75 / MTok |
| Prompt caching read | $0.30 / MTok |

## Extended Thinking with Tool Use

### Behavior Pattern
1. **First assistant turn**: 
   - Includes thinking blocks followed by tool use requests

2. **Tool result turn**:
   - Subsequent assistant message will not contain additional thinking blocks
   - No new thinking blocks until next non-`tool_result` user turn

### Normal Flow
1. User sends initial message
2. Assistant responds with thinking blocks and tool requests
3. User sends message with tool results
4. Assistant responds with more tool calls or text (no thinking blocks)
5. Repeat steps 3-4 if more tools needed

## Preserving Thinking Blocks

### Requirements
- Must pass `thinking` and `redacted_thinking` blocks back to API during tool use
- Include complete unmodified block back to API
- Critical for maintaining model's reasoning flow
- Cannot rearrange or modify block sequence

### API Handling
- Automatically filters provided thinking blocks
- Uses relevant blocks to preserve reasoning
- Only bills for input tokens shown to Claude

### Importance
1. **Reasoning Continuity**
   - Blocks capture step-by-step reasoning leading to tool requests
   - Including original thinking ensures continued reasoning from previous point

2. **Context Maintenance**
   - Tool results appear as user messages but part of continuous reasoning
   - Preserving blocks maintains conceptual flow across API calls

## Best Practices

1. **Budget Settings**
   - Start with 16,000+ tokens for complex tasks
   - Adjust based on task needs
   - Consider diminishing returns above 32K
   - Use batch processing for budgets above 32K

2. **Token Management**
   - Monitor usage for cost optimization
   - Previous thinking blocks automatically ignored
   - Adjust `max_tokens` as prompt length changes
   - Use token counting endpoints for accurate tracking

3. **Use Cases**
   - Complex tasks requiring step-by-step reasoning
   - Math problems
   - Coding tasks
   - Analysis work

4. **Performance Considerations**
   - Account for longer response times
   - Handle streaming appropriately
   - Be prepared for "chunky" delivery in streaming
   - Consider batch processing for large budgets

## Extended Output Capabilities (Beta)

- Support for up to 128K output tokens
- Enable with `anthropic-beta: output-128k-2025-02-19` header
- Useful for complex reasoning and comprehensive content
- Recommended to use streaming or batch mode

## Prompt Caching Considerations

### Cache Behavior
- Thinking blocks not meant to be cached
- Previous turn blocks ignored
- Disabled thinking content ignored by API

### Cache Invalidation
- Thinking parameter changes invalidate message cache points
- System prompts and tool definitions maintain caching
- Cache preserved when only thinking budget changes
